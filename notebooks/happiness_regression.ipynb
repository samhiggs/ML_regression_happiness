{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enable access to modules in project directory\n",
    "proj_root = (Path('..') / 'app').resolve()\n",
    "if proj_root not in sys.path:\n",
    "    sys.path.append(proj_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(proj_root / 'data' / 'happiness.csv', delimiter=',', header=0, skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the quantatitive columns. We are ignoring categorical data for this analysis\n",
    "cleaned_featureset = data.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "#Remove happiness_rank and happiness_score\n",
    "cleaned_featureset = cleaned_featureset.drop(columns=['happiness_rank', 'happiness_score'])\n",
    "\n",
    "#Convert 0's to NAN as there shouldn't be 0's in this dataset unless the value couldn't be calculated\n",
    "cleaned_featureset = cleaned_featureset.replace(0.0, np.NaN)\n",
    "\n",
    "#Replace the NA's with mean of column to keep consistent length\n",
    "cleaned_featureset = cleaned_featureset.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "\n",
    "#Add constant to columns with negative values so that all are positive\n",
    "cleaned_featureset = cleaned_featureset.apply(\n",
    "    lambda x: x.sub(x.min()-1) if x.min() < 0 else x)\n",
    "\n",
    "#Transform columns which have significant outliers and skewed data\n",
    "#Chose not to remove outliers as their information could still be significant\n",
    "transform_cube_cols = ['cellular_subscriptions', 'familiy_income_gini_coeff']\n",
    "transform_log_cols=['military_expenditures[%]', 'population', 'GDP_per_capita[$]', 'inflation_rate[%]']\n",
    "\n",
    "cleaned_featureset[transform_cube_cols] = cleaned_featureset[\n",
    "    transform_cube_cols].apply(lambda x: x.transform(np.cbrt))\n",
    "\n",
    "cleaned_featureset[transform_log_cols] = cleaned_featureset[\n",
    "    transform_log_cols].apply(lambda x: x.transform(np.log))\n",
    "cleaned_featureset['surplus_deficit_GDP[%]'] = cleaned_featureset['surplus_deficit_GDP[%]'].pow(3)\n",
    "#normalize data to fit between 0 and 1\n",
    "cleaned_featureset = cleaned_featureset.apply(\n",
    "    lambda x: [(el-x.min())/(x.max()-x.min()) for el in x], axis=0)\n",
    "\n",
    "#Nooow lets have a look at the sexy data | or not because latex outputs to PDF in ugly way...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_featureset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some of this data is still slightly skewed but the standardisation and transformation has done a pretty effective job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the happiness score back into the featureset.\n",
    "cleaned_featureset.insert(0, 'happiness_score', data['happiness_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Scatterplots\n",
    "The data from many of these scatterplots have already been normalized. Many of them reflect the contribution this study found in the variable's effect on the happiness score. The greater the score, the greater the effect on the survey respondents effect of happiness. \n",
    "\n",
    "## Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_w = 3\n",
    "plot_h = 5\n",
    "fig, ax = plt.subplots(plot_h, plot_w, sharey=\"row\")\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i in range(plot_h):\n",
    "    for j in range(plot_w):\n",
    "        feature = cleaned_featureset.columns[1+i*(plot_w)+j]\n",
    "        ax[i,j].scatter(cleaned_featureset[feature], cleaned_featureset['happiness_score'])\n",
    "        ax[i,j].set_title(feature)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We can see a range of correlations between features in our dataset and our happiness score. The model will use those deemed as most significant. The strongest features economy, family, health, GDP_per_capita and internet access population.\n",
    "There is a weaker correlation with the happiness score and inflation rate, cellular subscriptions and dystopia residual however it may be enough to improve our model, with an appropriate weighting assigned by the algorithm.\n",
    "\n",
    "It would seem intuitive that economy, family and health are all signficant as a thriving economy, people with family and a sense of belonging and a healthly state of being could improve a sense of happiness. A strong GDP per capita might keep people busy and an economy productive and internet access enable people to be connected to others as well as information. Counter to this, economic prosperity and higher GDP oer capita could result in higher levels of stressand higher suicide rates etc, High numbers of family could be due to high mortality in children, or requirements of support in farming (in third world countries) and internet can be used to spread disinformation and control populations. It is important to note that these studies are observational, surveys which are littered with personal bias. The factors for many of these variables are also abstracted and give us little insight into how they were calculated/weighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and set up packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the featureset that I am choosing to use.\n",
    "featureset_keys = ['economy', 'family', 'health', \n",
    "                   'freedom', 'dystopia_residual', \n",
    "                   'internet_access_population[%]', \n",
    "                   'cellular_subscriptions', 'GDP_per_capita[$]', \n",
    "                   'inflation_rate[%]'\n",
    "                  ]\n",
    "featureset = cleaned_featureset[featureset_keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing a correlation matrix from the featureset against the happiness score.\n",
    "plt.matshow(cleaned_featureset.corr())\n",
    "cleaned_featureset.corr()['happiness_score'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#KERNEL RIDGE LINEAR REGRESSION\n",
    "def train_model(df, featureset_keys, kernel=\"linear\", alpha=1.0, gamma=None, degree=None, coef0=None):\n",
    "\n",
    "    # Setup Parameters for Model\n",
    "    kr_args = {\"kernel\": kernel, \"alpha\": alpha}\n",
    "\n",
    "    # Validate parameters for polynomial\n",
    "    if kernel == \"polynomial\":\n",
    "        if degree is None or coef0 is None:\n",
    "            print(\"Must provide a parameter for degree and coef0\")\n",
    "            return None\n",
    "        else:\n",
    "            kr_args[\"gamma\"] = gamma\n",
    "            kr_args[\"degree\"] = degree\n",
    "            kr_args[\"coef0\"] = coef0\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    # Store the results of each training run\n",
    "    predictions = []\n",
    "    scores = []\n",
    "    \n",
    "    # Save the best model to return\n",
    "    best_model = None\n",
    "    baseline = 0.0\n",
    "    \n",
    "    i = 0\n",
    "    for train, test in RepeatedKFold(n_splits=5, n_repeats=1).split(df):\n",
    "        \n",
    "        # Split dataset\n",
    "        train_x, train_y = (df.iloc[train])[featureset_keys], (df.iloc[train])['happiness_score']\n",
    "        test_x, test_y =(df.iloc[test])[featureset_keys], (df.iloc[test])['happiness_score']\n",
    "        \n",
    "        # Initialise model\n",
    "        kr_model = KernelRidge(**kr_args)\n",
    "        \n",
    "        # Train model\n",
    "        kr_model.fit(train_x, train_y)\n",
    "        \n",
    "        # Evaluate model \n",
    "        pred_y = kr_model.predict(test_x)\n",
    "        score = kr_model.score(test_x, test_y)\n",
    "        \n",
    "        # Save if better then previous\n",
    "        if score > baseline:\n",
    "            best_model = kr_model\n",
    "        predictions.append(pred_y)\n",
    "        plt.scatter(test_y, p, label=f\"iter {i}\")\n",
    "        scores.append(score)\n",
    "        i = i+1\n",
    "        \n",
    "    plt.plot(df['happiness_score'], df['happiness_score'], label='actual')\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predictions')\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1.05, 1))\n",
    "    return best_model, plt, scores\n",
    "    print(pred_scores)\n",
    "    \n",
    "# best_model, plt, scores = train_model(cleaned_featureset, featureset_keys)\n",
    "best_model, plt, scores = train_model(cleaned_featureset, featureset_keys, kernel=\"polynomial\", degree=3, coef0=1)\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'kkr_model.pkl'\n",
    "pickle.dump(best_model, open(proj_root / 'model' / filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull serialized model\n",
    "loaded_model = pickle.load(open(proj_root / 'model' / filename, 'rb'))\n",
    "# generate predictions\n",
    "# economy, family, health, freedom, dystop., internet access, cellular subs, gdp, inflation\n",
    "result = loaded_model.predict([[1,1,1,1,1,1,1,1,1]])\n",
    "# Add prediction to db\n",
    "post_score(input, result, actual = None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "train_x, train_y = (cleaned_featureset.iloc[train])[featureset_keys], (cleaned_featureset.iloc[train])['happiness_score']\n",
    "components = pca.fit_transform(train_x)\n",
    "\n",
    "# If we wan to plot where all the countries lie in a PCA\n",
    "plt.scatter(components[:,0], components[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Results\n",
    "Running the Kernel Ridge model with a 1.2 alpha and using 5 fold cross validation, R^2 averaged at 0.95 plus/minus 0.01 in subsequent runs. The model runs with strong accuracy. The variation is mostly present in the lower happiness scores values. This could be due to less data being available or a lesser weight being placed on the available data that better predicts those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=[\"cross_validation_results\"])\n",
    "def train_model(df, features, alpha=1.0, kernel=\"linear\", gamma=None, degree=None, coef0=None):\n",
    "    \n",
    "    # Setup parameters for model\n",
    "    kr_args = {\"kernel\": kernel, \"alpha\": alpha, \"gamma\": gamma}        \n",
    "    \n",
    "    if kernel == \"polynomial\":\n",
    "        kr_args[\"degree\"] = degree \n",
    "        kr_args[\"coef0\"] = coef0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create and run cross validation\n",
    "    pred_list = []\n",
    "    pred_scores = []\n",
    "    for train, test in RepeatedKFold(n_splits=5, n_repeats=1).split(df):\n",
    "        \n",
    "        train_x, train_y = df.iloc[train][features], df.iloc[train]['happiness_score']\n",
    "        test_x, test_y = df.iloc[test][features], df.iloc[test]['happiness_score']\n",
    "        \n",
    "        print(train_x, test_x)\n",
    "        print(train_y, test_y)\n",
    "        # Initialise\n",
    "        kr_model = KernelRidge(**kr_args)\n",
    "        \n",
    "        # Train\n",
    "        fit = kr_model.fit(train_x, train_y)\n",
    "        \n",
    "        # Predict \n",
    "        prediction = kr_model.predict(test_x)\n",
    "        pred_list.append(prediction)\n",
    "        \n",
    "        # Evaluate\n",
    "        score = kr_model.score(test_x, test_y)\n",
    "        print(f\"Score for {kernel} model is {score}\")\n",
    "        pred_scores.append(score)\n",
    "        \n",
    "    for p in pred_list_quad:\n",
    "        plt.scatter(test_y, p)\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predictions')\n",
    "        \n",
    "    print(f'Predicted value average across 5 runs are {sum(pred_val_list_quad)/len(pred_val_list_quad):.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(cleaned_featureset, featureset_keys, \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KERNEL RIDGE POLYNOMIAL REGRESSION\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=1)\n",
    "pred_list_quad = []\n",
    "pred_val_list_quad = []\n",
    "for train, test in rkf.split(cleaned_featureset):\n",
    "    train_x, train_y = (cleaned_featureset.iloc[train])[featureset_keys],(cleaned_featureset.iloc[train])['happiness_score']\n",
    "    test_x, test_y = (cleaned_featureset.iloc[test])[featureset_keys], (cleaned_featureset.iloc[test])['happiness_score']\n",
    "    #Create model\n",
    "    clf_quad = KernelRidge(alpha=1.0, kernel='polynomial', gamma=1.3, degree=2, coef0=1.3)\n",
    "    #Build model with training data\n",
    "    fit_quad = clf_quad.fit(train_x, train_y)\n",
    "    #now use the model to \n",
    "    prediction = clf_quad.predict(test_x)\n",
    "    pred_list_quad.append(prediction)\n",
    "    score = clf_quad.score(test_x, test_y)\n",
    "    pred_val_list_quad.append(score)\n",
    "\n",
    "for p in pred_list_quad:\n",
    "    plt.scatter(test_y, p)\n",
    "\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "\n",
    "print('Predicted value average across 5 runs are {0:.4f}'.format(sum(pred_val_list_quad)/len(pred_val_list_quad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Results\n",
    "The algorithm used the Kernel Ridge polynomial model with alpha of 1.0, gamma of 1.3 and degrees of 2 (quadratic). R^2 across 5 fold cross validation averaged at 0.96 plus/minus 0.01 in subsequent runs. Like the linear model, the highest variance in the prediction occurred in the lower happiness scores. The model overpredicted earlier results initially, however varied between an over and underprediction. It was most accurate, and least varied around the mean and median of the happiness score which makes sense, as that is where most of the data lies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERNEL RIDGE GAUSSIAN REGRESSION\n",
    "\n",
    "rkf_g = RepeatedKFold(n_splits=5, n_repeats=1)\n",
    "pred_list_g = []\n",
    "pred_val_list_g = []\n",
    "for train, test in rkf_g.split(cleaned_featureset):\n",
    "    train_x, train_y = (cleaned_featureset.iloc[train])[featureset_keys],(cleaned_featureset.iloc[train])['happiness_score']\n",
    "    test_x, test_y = (cleaned_featureset.iloc[test])[featureset_keys], (cleaned_featureset.iloc[test])['happiness_score']\n",
    "    #Create model\n",
    "    clf = KernelRidge(alpha=.01, kernel='rbf', gamma=0.2)\n",
    "    #Build model with training data\n",
    "    fit = clf.fit(train_x, train_y)\n",
    "    #now use the model to \n",
    "    p = clf.predict(test_x)\n",
    "    pred_list_g.append(p)\n",
    "    s = clf.score(test_x, test_y)\n",
    "    pred_val_list_g.append(s)\n",
    "\n",
    "for p in pred_list_g:\n",
    "    plt.scatter(test_y, p)\n",
    "\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "\n",
    "print('Predicted value average across 5 runs are {0:.4f}'.format(sum(pred_val_list_g)/len(pred_val_list_g)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian algorithm used the Kernel Ridge radial basis function model with an alpha of 0.01 and gamma of 0.2. The $R^2$ average across 5 fold cross validation was 0.97 plus/minus 0.01. The model was most accurate using the gaussian model, likely because of the pre-processing work to normalize the data. Like the other two models, the highest variance occurred at the lowest happiness scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian model runs best ouut of the 3. This is likely because I normalized the data during pre-processing. Compared to my colleagues I am disappointed I didn't get a more accurate model however I think that this is within the range of accepatble measures and I would argue that the offset from truth would be because of a better generalization. However it is difficult to justify this model in any real cases due to the lack of quality of the data. I believe that happiness is such a subjective measure that any organisation building data on such a topic can only result in failure. I therefore urge anyone reading this report to be careful when referencing. I also urge the reader to seek other literature as there are bodies of longitudinal studies that challenge the null hypothesis that certain factors such as GDP are positively related to happiness. Whilst saying this, I would like to articulate that this opinion is outside of the scope of this report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "happiness-env",
   "language": "python",
   "name": "happiness-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
